{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z71WUV70iDTB"
      },
      "source": [
        "# PatchCoreViT Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparations"
      ],
      "metadata": {
        "id": "DLhiskAbcyEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_ubgP1ic_25",
        "outputId": "eca1ff75-ef43-4b7f-ee20-626f948897e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=174e9f1c380e6834a9b0643b638e7d2283efeb548b1ec82849d2374d13126ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import wget\n",
        "import json\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import patchcore_models\n",
        "import patchcore_utils\n",
        "\n",
        "from patchcore_utils import get_results, print_results, save_json\n",
        "from patchcore_models import MVTecDataset, PatchCoreViT, VanillaPatchCore, PatchCoreSWin\n",
        "\n",
        "class_links = {\n",
        "    \"bottle\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz\",\n",
        "    \"cable\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937413-1629951498/cable.tar.xz\",\n",
        "    \"capsule\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz\",\n",
        "    \"carpet\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz\",\n",
        "    \"grid\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937487-1629951814/grid.tar.xz\",\n",
        "    \"hazelnut\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz\",\n",
        "    \"leather\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937607-1629951964/leather.tar.xz\",\n",
        "    \"metal_nut\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937637-1629952063/metal_nut.tar.xz\",\n",
        "    \"pill\": \"https://www.mydrive.ch/shares/43421/11a215a5749fcfb75e331ddd5f8e43ee/download/420938129-1629953099/pill.tar.xz\",\n",
        "    \"screw\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz\",\n",
        "    \"tile\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938133-1629953189/tile.tar.xz\",\n",
        "    \"toothbrush\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938134-1629953256/toothbrush.tar.xz\",\n",
        "    \"transistor\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938166-1629953277/transistor.tar.xz\",\n",
        "    \"wood\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938383-1629953354/wood.tar.xz\",\n",
        "    \"zipper\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938385-1629953449/zipper.tar.xz\"\n",
        "}"
      ],
      "metadata": {
        "id": "hF1b11_qcwm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "c09590cc-aca2-4271-d35b-5a7ce1d14784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    \"capsule\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz\",\\n    \"carpet\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz\",\\n    \"grid\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937487-1629951814/grid.tar.xz\",\\n    \"hazelnut\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz\",\\n    \"leather\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937607-1629951964/leather.tar.xz\",\\n    \"metal_nut\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937637-1629952063/metal_nut.tar.xz\",\\n    \"pill\": \"https://www.mydrive.ch/shares/43421/11a215a5749fcfb75e331ddd5f8e43ee/download/420938129-1629953099/pill.tar.xz\",\\n    \"screw\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz\",\\n    \"tile\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938133-1629953189/tile.tar.xz\",\\n    \"toothbrush\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938134-1629953256/toothbrush.tar.xz\",\\n    \"transistor\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938166-1629953277/transistor.tar.xz\",\\n    \"wood\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938383-1629953354/wood.tar.xz\",\\n    \"zipper\": \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938385-1629953449/zipper.tar.xz\"\\n}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download duration 6m 5s\n",
        "for class_name in class_links:\n",
        "  os.mkdir(class_name)\n",
        "  file_path = wget.download(class_links[class_name], class_name)\n",
        "  if os.path.exists(file_path):\n",
        "    # Extract the file if it's a tar.xz file\n",
        "    if file_path.endswith('.tar.xz'):\n",
        "      shutil.unpack_archive(file_path, extract_dir=class_name)\n",
        "      os.remove(file_path)\n",
        "      print(f\"File {class_name} downloaded and extracted successfully.\")\n",
        "    else:\n",
        "      print(f\"Failed to download the file {class_name}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7H2NqpqcsX4",
        "outputId": "3e653612-1541-4e4e-8468-315455b26b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File bottle downloaded and extracted successfully.\n",
            "File cable downloaded and extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# remove\n",
        "# shutil.rmtree(\"bottle\")"
      ],
      "metadata": {
        "id": "nfYVH-o0kTk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Layer Analysis"
      ],
      "metadata": {
        "id": "QkM3gAn9cF9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 0-1-2"
      ],
      "metadata": {
        "id": "xyJPgVpwecOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 0\n",
        "print(f\"Layer: 0\")\n",
        "\n",
        "model_params = {\n",
        "  \"layers\" : [0],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "results = get_results(PatchCoreViT, c)\n",
        "print_results(results)\n",
        "result_json = save_json(results, \"pcViT_base-patch16-224-ink21k_l0.json\")\n",
        "# Avg AUC: 0.795 \t\tTotal Misclassified: 306"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g2316h3yefk5",
        "outputId": "5484f294-45dc-41db-cd43-f93ce2b3e866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 0\n",
            "\n",
            "Class: bottle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:12<00:00, 17.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 771.51it/s]\n",
            "100%|██████████| 83/83 [00:06<00:00, 12.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.988\n",
            "Val: PIXEL Level ROCAUC: 0.970\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Initial Score Threshold: 3.771 F1Score: 0.968\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.711 F1Score: 0.976\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.028s\n",
            "\n",
            "Class: cable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [00:19<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4389/4389 [00:05<00:00, 733.55it/s]\n",
            "100%|██████████| 150/150 [00:14<00:00, 10.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.477\n",
            "Val: PIXEL Level ROCAUC: 0.738\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.477\n",
            "[INFO][evaluate] Initial Score Threshold: 6.234 F1Score: 0.598\n",
            "[INFO][evaluate] Optimal Score Threshold: -3.766 F1Score: 0.760\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.026s\n",
            "\n",
            "Class: capsule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [00:18<00:00, 11.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4291/4291 [00:05<00:00, 731.47it/s]\n",
            "100%|██████████| 132/132 [00:12<00:00, 10.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.803\n",
            "Val: PIXEL Level ROCAUC: 0.913\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.803\n",
            "[INFO][evaluate] Initial Score Threshold: 2.257 F1Score: 0.874\n",
            "[INFO][evaluate] Optimal Score Threshold: 1.877 F1Score: 0.931\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.026s\n",
            "\n",
            "Class: carpet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 280/280 [00:23<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5487/5487 [00:09<00:00, 582.16it/s]\n",
            "100%|██████████| 117/117 [00:11<00:00, 10.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.525\n",
            "Val: PIXEL Level ROCAUC: 0.794\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.525\n",
            "[INFO][evaluate] Initial Score Threshold: 4.554 F1Score: 0.744\n",
            "[INFO][evaluate] Optimal Score Threshold: -5.446 F1Score: 0.864\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: grid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:12<00:00, 20.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5173/5173 [00:08<00:00, 616.19it/s]\n",
            "100%|██████████| 78/78 [00:04<00:00, 18.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.931\n",
            "Val: PIXEL Level ROCAUC: 0.956\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.931\n",
            "[INFO][evaluate] Initial Score Threshold: 4.205 F1Score: 0.885\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.025 F1Score: 0.922\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.016s\n",
            "\n",
            "Class: hazelnut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:31<00:00, 12.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7662/7662 [00:18<00:00, 419.67it/s]\n",
            "100%|██████████| 110/110 [00:10<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.884\n",
            "Val: PIXEL Level ROCAUC: 0.975\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.884\n",
            "[INFO][evaluate] Initial Score Threshold: 4.444 F1Score: 0.840\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.394 F1Score: 0.851\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: leather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:18<00:00, 13.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [00:07<00:00, 654.63it/s]\n",
            "100%|██████████| 124/124 [00:11<00:00, 10.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.780\n",
            "Val: PIXEL Level ROCAUC: 0.917\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.780\n",
            "[INFO][evaluate] Initial Score Threshold: 4.508 F1Score: 0.756\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.328 F1Score: 0.854\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: metal_nut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:10<00:00, 20.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4311/4311 [00:05<00:00, 726.08it/s]\n",
            "100%|██████████| 115/115 [00:06<00:00, 18.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.442\n",
            "Val: PIXEL Level ROCAUC: 0.830\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.442\n",
            "[INFO][evaluate] Initial Score Threshold: 4.652 F1Score: 0.432\n",
            "[INFO][evaluate] Optimal Score Threshold: -5.348 F1Score: 0.894\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.018s\n",
            "\n",
            "Class: pill\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 267/267 [00:18<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5232/5232 [00:08<00:00, 600.87it/s]\n",
            "100%|██████████| 167/167 [00:12<00:00, 13.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.899\n",
            "Val: PIXEL Level ROCAUC: 0.957\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.899\n",
            "[INFO][evaluate] Initial Score Threshold: 3.280 F1Score: 0.899\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.141 F1Score: 0.951\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: screw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:17<00:00, 18.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6271/6271 [00:12<00:00, 510.05it/s]\n",
            "100%|██████████| 160/160 [00:10<00:00, 15.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.717\n",
            "Val: PIXEL Level ROCAUC: 0.978\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.717\n",
            "[INFO][evaluate] Initial Score Threshold: 3.803 F1Score: 0.817\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.333 F1Score: 0.866\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: tile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [00:15<00:00, 15.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4507/4507 [00:06<00:00, 672.00it/s]\n",
            "100%|██████████| 117/117 [00:08<00:00, 13.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.811\n",
            "Val: PIXEL Level ROCAUC: 0.754\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.811\n",
            "[INFO][evaluate] Initial Score Threshold: 4.585 F1Score: 0.734\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.485 F1Score: 0.865\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: toothbrush\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:05<00:00, 11.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1175/1175 [00:00<00:00, 1955.85it/s]\n",
            "100%|██████████| 42/42 [00:03<00:00, 12.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.956\n",
            "Val: PIXEL Level ROCAUC: 0.980\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.956\n",
            "[INFO][evaluate] Initial Score Threshold: 4.315 F1Score: 0.893\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.215 F1Score: 0.933\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.017s\n",
            "\n",
            "Class: transistor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213/213 [00:19<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4173/4173 [00:05<00:00, 700.62it/s]\n",
            "100%|██████████| 100/100 [00:09<00:00, 10.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.799\n",
            "Val: PIXEL Level ROCAUC: 0.835\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.799\n",
            "[INFO][evaluate] Initial Score Threshold: 4.249 F1Score: 0.706\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.239 F1Score: 0.721\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: wood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247/247 [00:21<00:00, 11.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4840/4840 [00:07<00:00, 649.56it/s]\n",
            "100%|██████████| 79/79 [00:07<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.950\n",
            "Val: PIXEL Level ROCAUC: 0.902\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.950\n",
            "[INFO][evaluate] Initial Score Threshold: 4.129 F1Score: 0.911\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.920 F1Score: 0.935\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: zipper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 240/240 [00:11<00:00, 21.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4703/4703 [00:06<00:00, 682.03it/s]\n",
            "100%|██████████| 151/151 [00:09<00:00, 16.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.959\n",
            "Val: PIXEL Level ROCAUC: 0.935\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.959\n",
            "[INFO][evaluate] Initial Score Threshold: 4.054 F1Score: 0.949\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.944 F1Score: 0.971\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "CLASS BREAKDOWN\n",
            "ROCAUC: 0.988 \t\tf1_score: 0.976 \tbottle\n",
            "ROCAUC: 0.477 \t\tf1_score: 0.760 \tcable\n",
            "ROCAUC: 0.803 \t\tf1_score: 0.931 \tcapsule\n",
            "ROCAUC: 0.525 \t\tf1_score: 0.864 \tcarpet\n",
            "ROCAUC: 0.931 \t\tf1_score: 0.922 \tgrid\n",
            "ROCAUC: 0.884 \t\tf1_score: 0.851 \thazelnut\n",
            "ROCAUC: 0.780 \t\tf1_score: 0.854 \tleather\n",
            "ROCAUC: 0.442 \t\tf1_score: 0.894 \tmetal_nut\n",
            "ROCAUC: 0.899 \t\tf1_score: 0.951 \tpill\n",
            "ROCAUC: 0.717 \t\tf1_score: 0.866 \tscrew\n",
            "ROCAUC: 0.811 \t\tf1_score: 0.865 \ttile\n",
            "ROCAUC: 0.956 \t\tf1_score: 0.933 \ttoothbrush\n",
            "ROCAUC: 0.799 \t\tf1_score: 0.721 \ttransistor\n",
            "ROCAUC: 0.950 \t\tf1_score: 0.935 \twood\n",
            "ROCAUC: 0.959 \t\tf1_score: 0.971 \tzipper\n",
            "\n",
            "\n",
            "SUMMARY\n",
            "Avg AUC: 0.795 \t\tTotal Misclassified: 306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 1\n",
        "print(f\"Layer: 1\")\n",
        "\n",
        "model_params = {\n",
        "  \"layers\" : [1],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "results = get_results(PatchCoreViT, model_params)\n",
        "print_results(results)\n",
        "result_json = save_json(results, \"pcViT_base-patch16-224-ink21k_l1.json\")\n",
        "# Avg AUC: 0.933 \t\tTotal Misclassified: 183"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQAN_nhIqbiv",
        "outputId": "ece1ab99-ea80-412d-d83d-3255861dfbb0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 1\n",
            "\n",
            "Class: bottle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:11<00:00, 17.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 788.76it/s]\n",
            "100%|██████████| 83/83 [00:05<00:00, 14.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.999\n",
            "Val: PIXEL Level ROCAUC: 0.991\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.999\n",
            "[INFO][evaluate] Initial Score Threshold: 4.232 F1Score: 0.984\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.372 F1Score: 0.992\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.020s\n",
            "\n",
            "Class: cable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [00:18<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4389/4389 [00:05<00:00, 744.21it/s]\n",
            "100%|██████████| 150/150 [00:14<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.862\n",
            "Val: PIXEL Level ROCAUC: 0.942\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.862\n",
            "[INFO][evaluate] Initial Score Threshold: 5.180 F1Score: 0.833\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.151 F1Score: 0.834\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.027s\n",
            "\n",
            "Class: capsule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [00:18<00:00, 11.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4291/4291 [00:06<00:00, 692.64it/s]\n",
            "100%|██████████| 132/132 [00:12<00:00, 10.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.834\n",
            "Val: PIXEL Level ROCAUC: 0.930\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.834\n",
            "[INFO][evaluate] Initial Score Threshold: 3.204 F1Score: 0.856\n",
            "[INFO][evaluate] Optimal Score Threshold: 2.404 F1Score: 0.939\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.026s\n",
            "\n",
            "Class: carpet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 280/280 [00:22<00:00, 12.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5487/5487 [00:09<00:00, 582.22it/s]\n",
            "100%|██████████| 117/117 [00:10<00:00, 10.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.852\n",
            "Val: PIXEL Level ROCAUC: 0.967\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.852\n",
            "[INFO][evaluate] Initial Score Threshold: 4.492 F1Score: 0.815\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.242 F1Score: 0.889\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: grid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:12<00:00, 20.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5173/5173 [00:08<00:00, 629.33it/s]\n",
            "100%|██████████| 78/78 [00:04<00:00, 15.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.985\n",
            "Val: PIXEL Level ROCAUC: 0.976\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.985\n",
            "[INFO][evaluate] Initial Score Threshold: 4.299 F1Score: 0.954\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.159 F1Score: 0.965\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.020s\n",
            "\n",
            "Class: hazelnut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:31<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7662/7662 [00:18<00:00, 417.60it/s]\n",
            "100%|██████████| 110/110 [00:09<00:00, 11.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.985\n",
            "Val: PIXEL Level ROCAUC: 0.983\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.985\n",
            "[INFO][evaluate] Initial Score Threshold: 4.903 F1Score: 0.950\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.834 F1Score: 0.952\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: leather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:18<00:00, 13.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [00:07<00:00, 669.00it/s]\n",
            "100%|██████████| 124/124 [00:11<00:00, 11.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.995\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 4.448 F1Score: 0.995\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.438 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: metal_nut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:10<00:00, 21.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4311/4311 [00:07<00:00, 608.19it/s]\n",
            "100%|██████████| 115/115 [00:06<00:00, 16.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.973\n",
            "Val: PIXEL Level ROCAUC: 0.962\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.973\n",
            "[INFO][evaluate] Initial Score Threshold: 4.932 F1Score: 0.938\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.702 F1Score: 0.963\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.021s\n",
            "\n",
            "Class: pill\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 267/267 [00:15<00:00, 17.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5232/5232 [00:08<00:00, 608.65it/s]\n",
            "100%|██████████| 167/167 [00:11<00:00, 14.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.914\n",
            "Val: PIXEL Level ROCAUC: 0.944\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.914\n",
            "[INFO][evaluate] Initial Score Threshold: 4.111 F1Score: 0.901\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.852 F1Score: 0.936\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: screw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:15<00:00, 20.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6271/6271 [00:12<00:00, 516.48it/s]\n",
            "100%|██████████| 160/160 [00:09<00:00, 16.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.843\n",
            "Val: PIXEL Level ROCAUC: 0.964\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.843\n",
            "[INFO][evaluate] Initial Score Threshold: 3.964 F1Score: 0.824\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.684 F1Score: 0.882\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.021s\n",
            "\n",
            "Class: tile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [00:15<00:00, 15.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4507/4507 [00:06<00:00, 717.82it/s]\n",
            "100%|██████████| 117/117 [00:08<00:00, 13.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.935\n",
            "Val: PIXEL Level ROCAUC: 0.903\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.935\n",
            "[INFO][evaluate] Initial Score Threshold: 5.098 F1Score: 0.877\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.009 F1Score: 0.901\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: toothbrush\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 12.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1175/1175 [00:00<00:00, 2267.45it/s]\n",
            "100%|██████████| 42/42 [00:03<00:00, 12.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.975\n",
            "Val: PIXEL Level ROCAUC: 0.989\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.975\n",
            "[INFO][evaluate] Initial Score Threshold: 4.601 F1Score: 0.912\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.981 F1Score: 0.938\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.017s\n",
            "\n",
            "Class: transistor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213/213 [00:19<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4173/4173 [00:05<00:00, 750.84it/s]\n",
            "100%|██████████| 100/100 [00:09<00:00, 10.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.956\n",
            "Val: PIXEL Level ROCAUC: 0.937\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.956\n",
            "[INFO][evaluate] Initial Score Threshold: 4.560 F1Score: 0.878\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.550 F1Score: 0.892\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: wood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247/247 [00:20<00:00, 12.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4840/4840 [00:07<00:00, 652.01it/s]\n",
            "100%|██████████| 79/79 [00:06<00:00, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.968\n",
            "Val: PIXEL Level ROCAUC: 0.952\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.968\n",
            "[INFO][evaluate] Initial Score Threshold: 4.857 F1Score: 0.930\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.717 F1Score: 0.951\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "Class: zipper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 240/240 [00:11<00:00, 21.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4703/4703 [00:06<00:00, 683.33it/s]\n",
            "100%|██████████| 151/151 [00:08<00:00, 17.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.918\n",
            "Val: PIXEL Level ROCAUC: 0.961\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.918\n",
            "[INFO][evaluate] Initial Score Threshold: 3.930 F1Score: 0.927\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.550 F1Score: 0.952\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.020s\n",
            "\n",
            "CLASS BREAKDOWN\n",
            "ROCAUC: 0.999 \t\tf1_score: 0.992 \tbottle\n",
            "ROCAUC: 0.862 \t\tf1_score: 0.834 \tcable\n",
            "ROCAUC: 0.834 \t\tf1_score: 0.939 \tcapsule\n",
            "ROCAUC: 0.852 \t\tf1_score: 0.889 \tcarpet\n",
            "ROCAUC: 0.985 \t\tf1_score: 0.965 \tgrid\n",
            "ROCAUC: 0.985 \t\tf1_score: 0.952 \thazelnut\n",
            "ROCAUC: 1.000 \t\tf1_score: 1.000 \tleather\n",
            "ROCAUC: 0.973 \t\tf1_score: 0.963 \tmetal_nut\n",
            "ROCAUC: 0.914 \t\tf1_score: 0.936 \tpill\n",
            "ROCAUC: 0.843 \t\tf1_score: 0.882 \tscrew\n",
            "ROCAUC: 0.935 \t\tf1_score: 0.901 \ttile\n",
            "ROCAUC: 0.975 \t\tf1_score: 0.938 \ttoothbrush\n",
            "ROCAUC: 0.956 \t\tf1_score: 0.892 \ttransistor\n",
            "ROCAUC: 0.968 \t\tf1_score: 0.951 \twood\n",
            "ROCAUC: 0.918 \t\tf1_score: 0.952 \tzipper\n",
            "\n",
            "\n",
            "SUMMARY\n",
            "Avg AUC: 0.933 \t\tTotal Misclassified: 183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 2\n",
        "print(f\"Layer: 2\")\n",
        "\n",
        "model_params = {\n",
        "  \"layers\" : [2],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "results = get_results(PatchCoreViT, model_params)\n",
        "print_results(results)\n",
        "result_json = save_json(results, \"pcViT_base-patch16-224-ink21k_l2.json\")\n",
        "# Avg AUC: 0.959 \t\tTotal Misclassified: 116"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "23838b2c237a460da56d2e38751e804c",
            "c3e94b46157e4aea8c02c408f44bd4b0",
            "d11e4451de6a44b485ae14448b26a31e",
            "1dc17249a5a344bfb97cddc92b11c83a",
            "3537e79038054ad8a14a269c1eca80c6",
            "a34af98a144d42198c55edf54dd259ae",
            "ffd7dadbe81a40108fa0a918006c9b4f",
            "d10200e62fd24c8e8bdc8f14d2c4b433",
            "385b18f78a2743bcb6c9ec9f177ecd36",
            "18e7e7d959db4a94ac9d03465e1c801e",
            "c960e9fe149c4377b665d33c9e4315bc",
            "c19fd6c08e244afb9ead593af801e880",
            "778eaab52e2c4d9598596c6d67611bf1",
            "8f8d649046f64c63a696ec85d29436aa",
            "3cdf89789234494f9b3f209c7c2a4152",
            "7779dce1d9b44857ba99874eecc6e224",
            "fd763229c2ff4ab58eb6d9d4966c978a",
            "a3d77da52d3745d78530ed02514c19db",
            "6ecc6c7e44bd48a19655f9175da1f4af",
            "a47998d587fe49b48faa5ffb23d18fd4",
            "4ea5e774f0914b75bb12622ae017ef6d",
            "18e36b1b2b5b48f0b8f86f47f6fc93fc",
            "406f126b1bff446d86b5a4f7dec8ea42",
            "f49e05852556416489674a6351a109ca",
            "17b43a5e0e4e4af7827b649256ea4842",
            "4d797d6b237b43cab8e287e0b5877609",
            "5fd90c7d73414f1989293f0ae3f00290",
            "e173d007180c4f55a7b9f1cd70bd64e4",
            "8f378a0ece9b4317b5717ef7665cd2e6",
            "775dcc565bd3459e8214d63d1d40512e",
            "6865f73dc0a948d3b2bfdb73cd1ecd20",
            "00ca736408c6418984e958bce389a4fe",
            "1c49f3a308f84763adb8dad5e7df04a5"
          ]
        },
        "collapsed": true,
        "id": "2rSak_ea1yHV",
        "outputId": "d076f456-b9dc-44cd-c275-762305072a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 2\n",
            "\n",
            "Class: bottle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23838b2c237a460da56d2e38751e804c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c19fd6c08e244afb9ead593af801e880"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "406f126b1bff446d86b5a4f7dec8ea42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:13<00:00, 15.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 760.07it/s]\n",
            "100%|██████████| 83/83 [00:05<00:00, 14.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.999\n",
            "Val: PIXEL Level ROCAUC: 0.991\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.999\n",
            "[INFO][evaluate] Initial Score Threshold: 4.870 F1Score: 0.984\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.550 F1Score: 0.992\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.026s\n",
            "\n",
            "Class: cable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [00:18<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4389/4389 [00:06<00:00, 731.05it/s]\n",
            "100%|██████████| 150/150 [00:13<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.967\n",
            "Val: PIXEL Level ROCAUC: 0.969\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.967\n",
            "[INFO][evaluate] Initial Score Threshold: 5.256 F1Score: 0.934\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.226 F1Score: 0.935\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: capsule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [00:18<00:00, 12.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4291/4291 [00:05<00:00, 749.16it/s]\n",
            "100%|██████████| 132/132 [00:11<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.897\n",
            "Val: PIXEL Level ROCAUC: 0.927\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.897\n",
            "[INFO][evaluate] Initial Score Threshold: 3.376 F1Score: 0.887\n",
            "[INFO][evaluate] Optimal Score Threshold: 2.646 F1Score: 0.936\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.026s\n",
            "\n",
            "Class: carpet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 280/280 [00:22<00:00, 12.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5487/5487 [00:09<00:00, 590.56it/s]\n",
            "100%|██████████| 117/117 [00:10<00:00, 11.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.952\n",
            "Val: PIXEL Level ROCAUC: 0.989\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.952\n",
            "[INFO][evaluate] Initial Score Threshold: 4.692 F1Score: 0.929\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.633 F1Score: 0.936\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: grid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:12<00:00, 21.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5173/5173 [00:08<00:00, 626.50it/s]\n",
            "100%|██████████| 78/78 [00:04<00:00, 18.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.992\n",
            "Val: PIXEL Level ROCAUC: 0.977\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.992\n",
            "[INFO][evaluate] Initial Score Threshold: 4.248 F1Score: 0.964\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.078 F1Score: 0.973\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.016s\n",
            "\n",
            "Class: hazelnut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:30<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7662/7662 [00:18<00:00, 415.47it/s]\n",
            "100%|██████████| 110/110 [00:09<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.990\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 5.450 F1Score: 0.986\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.280 F1Score: 0.993\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: leather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:17<00:00, 13.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [00:07<00:00, 668.31it/s]\n",
            "100%|██████████| 124/124 [00:10<00:00, 11.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.995\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 4.728 F1Score: 0.995\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.428 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: metal_nut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:09<00:00, 22.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4311/4311 [00:05<00:00, 740.34it/s]\n",
            "100%|██████████| 115/115 [00:05<00:00, 19.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.998\n",
            "Val: PIXEL Level ROCAUC: 0.964\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.998\n",
            "[INFO][evaluate] Initial Score Threshold: 5.308 F1Score: 0.989\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.258 F1Score: 0.995\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.017s\n",
            "\n",
            "Class: pill\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 267/267 [00:15<00:00, 17.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5232/5232 [00:08<00:00, 612.47it/s]\n",
            "100%|██████████| 167/167 [00:11<00:00, 14.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.946\n",
            "Val: PIXEL Level ROCAUC: 0.957\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.946\n",
            "[INFO][evaluate] Initial Score Threshold: 4.448 F1Score: 0.933\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.149 F1Score: 0.938\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "Class: screw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:15<00:00, 21.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6271/6271 [00:12<00:00, 520.90it/s]\n",
            "100%|██████████| 160/160 [00:09<00:00, 16.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.821\n",
            "Val: PIXEL Level ROCAUC: 0.934\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.821\n",
            "[INFO][evaluate] Initial Score Threshold: 4.203 F1Score: 0.802\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.973 F1Score: 0.888\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.021s\n",
            "\n",
            "Class: tile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [00:14<00:00, 15.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4507/4507 [00:06<00:00, 720.75it/s]\n",
            "100%|██████████| 117/117 [00:08<00:00, 13.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.955\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 5.582 F1Score: 0.994\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.562 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "Class: toothbrush\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 12.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1175/1175 [00:00<00:00, 2209.77it/s]\n",
            "100%|██████████| 42/42 [00:03<00:00, 13.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.919\n",
            "Val: PIXEL Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.919\n",
            "[INFO][evaluate] Initial Score Threshold: 4.389 F1Score: 0.935\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.179 F1Score: 0.952\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.016s\n",
            "\n",
            "Class: transistor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213/213 [00:17<00:00, 12.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4173/4173 [00:05<00:00, 757.43it/s]\n",
            "100%|██████████| 100/100 [00:08<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.992\n",
            "Val: PIXEL Level ROCAUC: 0.942\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.992\n",
            "[INFO][evaluate] Initial Score Threshold: 5.127 F1Score: 0.935\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.017 F1Score: 0.937\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: wood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247/247 [00:20<00:00, 12.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4840/4840 [00:07<00:00, 670.50it/s]\n",
            "100%|██████████| 79/79 [00:07<00:00, 10.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.985\n",
            "Val: PIXEL Level ROCAUC: 0.964\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.985\n",
            "[INFO][evaluate] Initial Score Threshold: 5.612 F1Score: 0.966\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.552 F1Score: 0.975\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: zipper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 240/240 [00:10<00:00, 22.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4703/4703 [00:06<00:00, 689.96it/s]\n",
            "100%|██████████| 151/151 [00:08<00:00, 17.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.923\n",
            "Val: PIXEL Level ROCAUC: 0.939\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.923\n",
            "[INFO][evaluate] Initial Score Threshold: 4.043 F1Score: 0.912\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.833 F1Score: 0.938\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.019s\n",
            "\n",
            "CLASS BREAKDOWN\n",
            "ROCAUC: 0.999 \t\tf1_score: 0.992 \tbottle\n",
            "ROCAUC: 0.967 \t\tf1_score: 0.935 \tcable\n",
            "ROCAUC: 0.897 \t\tf1_score: 0.936 \tcapsule\n",
            "ROCAUC: 0.952 \t\tf1_score: 0.936 \tcarpet\n",
            "ROCAUC: 0.992 \t\tf1_score: 0.973 \tgrid\n",
            "ROCAUC: 1.000 \t\tf1_score: 0.993 \thazelnut\n",
            "ROCAUC: 1.000 \t\tf1_score: 1.000 \tleather\n",
            "ROCAUC: 0.998 \t\tf1_score: 0.995 \tmetal_nut\n",
            "ROCAUC: 0.946 \t\tf1_score: 0.938 \tpill\n",
            "ROCAUC: 0.821 \t\tf1_score: 0.888 \tscrew\n",
            "ROCAUC: 1.000 \t\tf1_score: 1.000 \ttile\n",
            "ROCAUC: 0.919 \t\tf1_score: 0.952 \ttoothbrush\n",
            "ROCAUC: 0.992 \t\tf1_score: 0.937 \ttransistor\n",
            "ROCAUC: 0.985 \t\tf1_score: 0.975 \twood\n",
            "ROCAUC: 0.923 \t\tf1_score: 0.938 \tzipper\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3366881460.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatchCoreViT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msave_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pcViT_base-patch16-224-ink21k_l2.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Avg AUC: ???          Total Misclassified: ???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Utility.py\u001b[0m in \u001b[0;36mprint_results\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mclassName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ROCAUC: {result['auc']:.3f} \\t\\tf1_score: {result['prfs'][2]:.3f} \\t{className}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSUMMARY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 3\n",
        "print(f\"Layer: 3\")\n",
        "\n",
        "model_params = {\n",
        "  \"layers\" : [3],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "results = get_results(PatchCoreViT, model_params)\n",
        "print_results(results)\n",
        "result_json = save_json(results, \"pcViT_base-patch16-224-ink21k_l3.json\")\n",
        "# Avg AUC: 0.962 \t\tTotal Misclassified: 98"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L43QPvEU9hbN",
        "outputId": "70aca50f-9da0-4e74-cad9-457a044bbec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 3\n",
            "\n",
            "Class: bottle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:11<00:00, 17.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 778.51it/s]\n",
            "100%|██████████| 83/83 [00:05<00:00, 16.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.999\n",
            "Val: PIXEL Level ROCAUC: 0.990\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.999\n",
            "[INFO][evaluate] Initial Score Threshold: 5.494 F1Score: 0.984\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.954 F1Score: 0.992\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.018s\n",
            "\n",
            "Class: cable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [00:18<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4389/4389 [00:05<00:00, 738.11it/s]\n",
            "100%|██████████| 150/150 [00:13<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.978\n",
            "Val: PIXEL Level ROCAUC: 0.977\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.978\n",
            "[INFO][evaluate] Initial Score Threshold: 5.880 F1Score: 0.951\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.780 F1Score: 0.957\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: capsule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [00:17<00:00, 12.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4291/4291 [00:05<00:00, 746.83it/s]\n",
            "100%|██████████| 132/132 [00:11<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.920\n",
            "Val: PIXEL Level ROCAUC: 0.938\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.920\n",
            "[INFO][evaluate] Initial Score Threshold: 3.687 F1Score: 0.914\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.087 F1Score: 0.947\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: carpet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 280/280 [00:22<00:00, 12.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5487/5487 [00:09<00:00, 586.89it/s]\n",
            "100%|██████████| 117/117 [00:10<00:00, 11.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.967\n",
            "Val: PIXEL Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.967\n",
            "[INFO][evaluate] Initial Score Threshold: 5.632 F1Score: 0.953\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.602 F1Score: 0.959\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: grid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:12<00:00, 21.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5173/5173 [00:08<00:00, 623.14it/s]\n",
            "100%|██████████| 78/78 [00:03<00:00, 19.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.988\n",
            "Val: PIXEL Level ROCAUC: 0.977\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Initial Score Threshold: 4.674 F1Score: 0.954\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.564 F1Score: 0.964\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.016s\n",
            "\n",
            "Class: hazelnut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:30<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7662/7662 [00:18<00:00, 422.28it/s]\n",
            "100%|██████████| 110/110 [00:09<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.991\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 6.251 F1Score: 0.993\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.071 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: leather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:17<00:00, 13.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [00:07<00:00, 662.30it/s]\n",
            "100%|██████████| 124/124 [00:09<00:00, 12.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.995\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 6.432 F1Score: 0.995\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.962 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: metal_nut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:09<00:00, 22.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4311/4311 [00:05<00:00, 745.17it/s]\n",
            "100%|██████████| 115/115 [00:06<00:00, 18.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.999\n",
            "Val: PIXEL Level ROCAUC: 0.969\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.999\n",
            "[INFO][evaluate] Initial Score Threshold: 5.937 F1Score: 0.989\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.797 F1Score: 0.995\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.019s\n",
            "\n",
            "Class: pill\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 267/267 [00:15<00:00, 17.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5232/5232 [00:08<00:00, 607.82it/s]\n",
            "100%|██████████| 167/167 [00:10<00:00, 15.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.956\n",
            "Val: PIXEL Level ROCAUC: 0.965\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.956\n",
            "[INFO][evaluate] Initial Score Threshold: 5.166 F1Score: 0.945\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.986 F1Score: 0.957\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "Class: screw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:15<00:00, 21.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6271/6271 [00:12<00:00, 517.70it/s]\n",
            "100%|██████████| 160/160 [00:08<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.799\n",
            "Val: PIXEL Level ROCAUC: 0.937\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.799\n",
            "[INFO][evaluate] Initial Score Threshold: 4.809 F1Score: 0.822\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.509 F1Score: 0.865\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.019s\n",
            "\n",
            "Class: tile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [00:14<00:00, 16.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4507/4507 [00:06<00:00, 708.64it/s]\n",
            "100%|██████████| 117/117 [00:08<00:00, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.968\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 6.297 F1Score: 0.994\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.237 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.021s\n",
            "\n",
            "Class: toothbrush\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 12.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1175/1175 [00:00<00:00, 2000.25it/s]\n",
            "100%|██████████| 42/42 [00:03<00:00, 12.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.911\n",
            "Val: PIXEL Level ROCAUC: 0.987\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.911\n",
            "[INFO][evaluate] Initial Score Threshold: 4.683 F1Score: 0.935\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.544 F1Score: 0.952\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.017s\n",
            "\n",
            "Class: transistor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213/213 [00:18<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4173/4173 [00:05<00:00, 771.64it/s]\n",
            "100%|██████████| 100/100 [00:09<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.994\n",
            "Val: PIXEL Level ROCAUC: 0.967\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.994\n",
            "[INFO][evaluate] Initial Score Threshold: 5.643 F1Score: 0.951\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.623 F1Score: 0.964\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: wood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247/247 [00:19<00:00, 12.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4840/4840 [00:07<00:00, 657.81it/s]\n",
            "100%|██████████| 79/79 [00:06<00:00, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.987\n",
            "Val: PIXEL Level ROCAUC: 0.970\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.987\n",
            "[INFO][evaluate] Initial Score Threshold: 6.831 F1Score: 0.966\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.771 F1Score: 0.974\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "Class: zipper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 240/240 [00:10<00:00, 21.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4703/4703 [00:06<00:00, 681.51it/s]\n",
            "100%|██████████| 151/151 [00:08<00:00, 18.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.929\n",
            "Val: PIXEL Level ROCAUC: 0.908\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.929\n",
            "[INFO][evaluate] Initial Score Threshold: 4.411 F1Score: 0.922\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.181 F1Score: 0.964\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.019s\n",
            "\n",
            "CLASS BREAKDOWN\n",
            "ROCAUC: 0.999 \t\tf1_score: 0.992 \tbottle\n",
            "ROCAUC: 0.978 \t\tf1_score: 0.957 \tcable\n",
            "ROCAUC: 0.920 \t\tf1_score: 0.947 \tcapsule\n",
            "ROCAUC: 0.967 \t\tf1_score: 0.959 \tcarpet\n",
            "ROCAUC: 0.988 \t\tf1_score: 0.964 \tgrid\n",
            "ROCAUC: 1.000 \t\tf1_score: 1.000 \thazelnut\n",
            "ROCAUC: 1.000 \t\tf1_score: 1.000 \tleather\n",
            "ROCAUC: 0.999 \t\tf1_score: 0.995 \tmetal_nut\n",
            "ROCAUC: 0.956 \t\tf1_score: 0.957 \tpill\n",
            "ROCAUC: 0.799 \t\tf1_score: 0.865 \tscrew\n",
            "ROCAUC: 1.000 \t\tf1_score: 1.000 \ttile\n",
            "ROCAUC: 0.911 \t\tf1_score: 0.952 \ttoothbrush\n",
            "ROCAUC: 0.994 \t\tf1_score: 0.964 \ttransistor\n",
            "ROCAUC: 0.987 \t\tf1_score: 0.974 \twood\n",
            "ROCAUC: 0.929 \t\tf1_score: 0.964 \tzipper\n",
            "\n",
            "SUMMARY\n",
            "Avg AUC: 0.962 \t\tTotal Misclassified: 98\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bottle': {'cm': [[19, 1], [0, 63]],\n",
              "  'prfs': (0.984375, 1.0, 0.9921259842519685, None),\n",
              "  'auc': np.float64(0.9992063492063492)},\n",
              " 'cable': {'cm': [[54, 4], [4, 88]],\n",
              "  'prfs': (0.9565217391304348, 0.9565217391304348, 0.9565217391304348, None),\n",
              "  'auc': np.float64(0.9778860569715142)},\n",
              " 'capsule': {'cm': [[12, 11], [1, 108]],\n",
              "  'prfs': (0.907563025210084, 0.9908256880733946, 0.9473684210526315, None),\n",
              "  'auc': np.float64(0.9202233745512565)},\n",
              " 'carpet': {'cm': [[28, 0], [7, 82]],\n",
              "  'prfs': (1.0, 0.9213483146067416, 0.9590643274853801, None),\n",
              "  'auc': np.float64(0.9674959871589085)},\n",
              " 'grid': {'cm': [[20, 1], [3, 54]],\n",
              "  'prfs': (0.9818181818181818, 0.9473684210526315, 0.9642857142857143, None),\n",
              "  'auc': np.float64(0.9883040935672514)},\n",
              " 'hazelnut': {'cm': [[40, 0], [0, 70]],\n",
              "  'prfs': (1.0, 1.0, 1.0, None),\n",
              "  'auc': np.float64(1.0)},\n",
              " 'leather': {'cm': [[32, 0], [0, 92]],\n",
              "  'prfs': (1.0, 1.0, 1.0, None),\n",
              "  'auc': np.float64(1.0)},\n",
              " 'metal_nut': {'cm': [[22, 0], [1, 92]],\n",
              "  'prfs': (1.0, 0.989247311827957, 0.9945945945945946, None),\n",
              "  'auc': np.float64(0.998533724340176)},\n",
              " 'pill': {'cm': [[21, 5], [7, 134]],\n",
              "  'prfs': (0.9640287769784173, 0.950354609929078, 0.9571428571428572, None),\n",
              "  'auc': np.float64(0.9560829241680304)},\n",
              " 'screw': {'cm': [[13, 28], [7, 112]],\n",
              "  'prfs': (0.8, 0.9411764705882353, 0.8648648648648649, None),\n",
              "  'auc': np.float64(0.7985242877638861)},\n",
              " 'tile': {'cm': [[33, 0], [0, 84]],\n",
              "  'prfs': (1.0, 1.0, 1.0, None),\n",
              "  'auc': np.float64(1.0)},\n",
              " 'toothbrush': {'cm': [[9, 3], [0, 30]],\n",
              "  'prfs': (0.9090909090909091, 1.0, 0.9523809523809523, None),\n",
              "  'auc': np.float64(0.9111111111111111)},\n",
              " 'transistor': {'cm': [[57, 3], [0, 40]],\n",
              "  'prfs': (0.9302325581395349, 1.0, 0.963855421686747, None),\n",
              "  'auc': np.float64(0.9937499999999999)},\n",
              " 'wood': {'cm': [[19, 0], [3, 57]],\n",
              "  'prfs': (1.0, 0.95, 0.9743589743589743, None),\n",
              "  'auc': np.float64(0.9868421052631579)},\n",
              " 'zipper': {'cm': [[23, 9], [0, 119]],\n",
              "  'prfs': (0.9296875, 1.0, 0.9635627530364372, None),\n",
              "  'auc': np.float64(0.929359243697479)},\n",
              " 'average': 0.9618212838532748,\n",
              " 'misclassified': 98,\n",
              " 'model_params': {'layers': [3],\n",
              "  'backbone': 'google/vit-base-patch16-224-in21k',\n",
              "  'f_coreset': 0.1}}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 4\n",
        "print(f\"Layer: 4\")\n",
        "\n",
        "model_params = {\n",
        "  \"layers\" : [4],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "results = get_results(PatchCoreViT, model_params)\n",
        "print_results(results)\n",
        "result_json = save_json(results, \"pcViT_base-patch16-224-ink21k_l4.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jn-FpynMKzlP",
        "outputId": "dcfd70b1-00a5-4599-ec45-ea92e20dc47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 4\n",
            "\n",
            "Class: bottle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:11<00:00, 17.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 791.49it/s]\n",
            "100%|██████████| 83/83 [00:05<00:00, 15.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 4.801 F1Score: 0.992\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.711 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.018s\n",
            "\n",
            "Class: cable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [00:18<00:00, 12.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4389/4389 [00:05<00:00, 747.46it/s]\n",
            "100%|██████████| 150/150 [00:13<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.985\n",
            "Val: PIXEL Level ROCAUC: 0.980\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.985\n",
            "[INFO][evaluate] Initial Score Threshold: 6.101 F1Score: 0.956\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.101 F1Score: 0.962\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: capsule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [00:17<00:00, 12.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4291/4291 [00:05<00:00, 743.38it/s]\n",
            "100%|██████████| 132/132 [00:12<00:00, 10.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.921\n",
            "Val: PIXEL Level ROCAUC: 0.937\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.921\n",
            "[INFO][evaluate] Initial Score Threshold: 3.741 F1Score: 0.919\n",
            "[INFO][evaluate] Optimal Score Threshold: 3.521 F1Score: 0.946\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: carpet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 280/280 [00:21<00:00, 12.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5487/5487 [00:09<00:00, 595.97it/s]\n",
            "100%|██████████| 117/117 [00:10<00:00, 11.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.960\n",
            "Val: PIXEL Level ROCAUC: 0.986\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.960\n",
            "[INFO][evaluate] Initial Score Threshold: 5.874 F1Score: 0.953\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.864 F1Score: 0.959\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n",
            "\n",
            "Class: grid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264/264 [00:12<00:00, 21.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5173/5173 [00:08<00:00, 624.17it/s]\n",
            "100%|██████████| 78/78 [00:03<00:00, 20.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.991\n",
            "Val: PIXEL Level ROCAUC: 0.974\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.991\n",
            "[INFO][evaluate] Initial Score Threshold: 5.346 F1Score: 0.954\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.706 F1Score: 0.966\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.016s\n",
            "\n",
            "Class: hazelnut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:30<00:00, 12.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7662/7662 [00:18<00:00, 420.36it/s]\n",
            "100%|██████████| 110/110 [00:09<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.992\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 6.798 F1Score: 0.993\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.758 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: leather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:17<00:00, 13.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [00:07<00:00, 669.92it/s]\n",
            "100%|██████████| 124/124 [00:10<00:00, 11.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.995\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 7.350 F1Score: 0.995\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.140 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.024s\n",
            "\n",
            "Class: metal_nut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:09<00:00, 22.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4311/4311 [00:05<00:00, 748.97it/s]\n",
            "100%|██████████| 115/115 [00:06<00:00, 18.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.999\n",
            "Val: PIXEL Level ROCAUC: 0.976\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.999\n",
            "[INFO][evaluate] Initial Score Threshold: 6.243 F1Score: 0.989\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.193 F1Score: 0.995\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.018s\n",
            "\n",
            "Class: pill\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 267/267 [00:15<00:00, 17.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5232/5232 [00:08<00:00, 617.14it/s]\n",
            "100%|██████████| 167/167 [00:11<00:00, 14.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.944\n",
            "Val: PIXEL Level ROCAUC: 0.975\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.944\n",
            "[INFO][evaluate] Initial Score Threshold: 5.705 F1Score: 0.941\n",
            "[INFO][evaluate] Optimal Score Threshold: 5.485 F1Score: 0.957\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: screw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 320/320 [00:14<00:00, 21.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6271/6271 [00:12<00:00, 522.44it/s]\n",
            "100%|██████████| 160/160 [00:09<00:00, 17.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.773\n",
            "Val: PIXEL Level ROCAUC: 0.942\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.773\n",
            "[INFO][evaluate] Initial Score Threshold: 5.178 F1Score: 0.804\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.759 F1Score: 0.878\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.020s\n",
            "\n",
            "Class: tile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [00:13<00:00, 16.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4507/4507 [00:06<00:00, 707.53it/s]\n",
            "100%|██████████| 117/117 [00:07<00:00, 15.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.967\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 6.838 F1Score: 0.994\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.838 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.020s\n",
            "\n",
            "Class: toothbrush\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1175/1175 [00:00<00:00, 2280.80it/s]\n",
            "100%|██████████| 42/42 [00:03<00:00, 12.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.931\n",
            "Val: PIXEL Level ROCAUC: 0.986\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.931\n",
            "[INFO][evaluate] Initial Score Threshold: 5.136 F1Score: 0.935\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.766 F1Score: 0.952\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.017s\n",
            "\n",
            "Class: transistor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213/213 [00:17<00:00, 12.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4173/4173 [00:05<00:00, 758.74it/s]\n",
            "100%|██████████| 100/100 [00:08<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.988\n",
            "Val: PIXEL Level ROCAUC: 0.966\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Initial Score Threshold: 6.475 F1Score: 0.933\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.455 F1Score: 0.947\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.023s\n",
            "\n",
            "Class: wood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247/247 [00:19<00:00, 12.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4840/4840 [00:07<00:00, 659.61it/s]\n",
            "100%|██████████| 79/79 [00:06<00:00, 11.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.988\n",
            "Val: PIXEL Level ROCAUC: 0.970\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Initial Score Threshold: 7.760 F1Score: 0.947\n",
            "[INFO][evaluate] Optimal Score Threshold: 7.070 F1Score: 0.968\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.022s\n",
            "\n",
            "Class: zipper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 240/240 [00:10<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4703/4703 [00:06<00:00, 697.18it/s]\n",
            "100%|██████████| 151/151 [00:08<00:00, 17.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.947\n",
            "Val: PIXEL Level ROCAUC: 0.880\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.947\n",
            "[INFO][evaluate] Initial Score Threshold: 4.895 F1Score: 0.921\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.345 F1Score: 0.956\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.019s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'avg_auc_img' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-262964248.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatchCoreViT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mresult_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pcViT_base-patch16-224-ink21k_l4.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Utility.py\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(model_constructor, model_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mavg_pxl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_pxl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auc_pxl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m   \u001b[0mavg_auc_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_auc_img\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m   \u001b[0mavg_auc_pxl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_auc_pxl\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'avg_auc_img' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import patchcore_models\n",
        "import patchcore_utils\n",
        "\n",
        "importlib.reload(patchcore_utils)\n",
        "importlib.reload(patchcore_models)\n",
        "\n",
        "from patchcore_utils import get_results, print_results, save_json\n",
        "from patchcore_models import MVTecDataset, PatchCoreViT, VanillaPatchCore, PatchCoreSWin\n",
        "\n",
        "# print_results(results)\n",
        "# save_json(results, \"pcViT_base-patch16-224-ink21k_l2.json\")"
      ],
      "metadata": {
        "id": "ZI_abVTC6K0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Development\n",
        "\n"
      ],
      "metadata": {
        "id": "q-AEp6kAcVNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importlib.reload(patchcore_utils)\n",
        "importlib.reload(patchcore_models)\n",
        "\n",
        "from patchcore_utils import get_results, print_results, save_json\n",
        "from patchcore_models import MVTecDataset, PatchCoreViT, VanillaPatchCore, PatchCoreSWin"
      ],
      "metadata": {
        "id": "shxJS-n-MDmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "  \"layers\" : [4],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "base_path = \"/content/\"\n",
        "class_name = \"bottle\"\n",
        "\n",
        "temp_path = os.path.join(base_path, class_name, class_name) # ex. /content/bottle/bottle\n",
        "train_path, test_path  = os.path.join(temp_path, \"train\", \"good\"), os.path.join(temp_path, \"test\")\n",
        "train_paths, test_paths= [train_path], [os.path.join(test_path, path) for path in os.listdir(test_path)]\n",
        "\n",
        "pcViT = PatchCoreViT(**model_params)\n",
        "pcViT.fit(train_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEPmMmjjV7nr",
        "outputId": "a8bf2850-b9ae-48c6-adad-b10a2a2afdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:11<00:00, 18.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 783.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = pcViT.get_dataloader(test_paths)\n",
        "\n",
        "for sample_evaluate, label, path, mask in test_dataloader:\n",
        "    break\n",
        "\n",
        "sample_ale = pcViT.process_input(path[0])\n",
        "\n",
        "print(sample_evaluate.pixel_values[0].shape)\n",
        "print(sample_ale.pixel_values[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_jx5RI1WQKw",
        "outputId": "d194df2c-7a25-486c-dd96-5c9a97345ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 4\n",
        "print(f\"Layer: 4\")\n",
        "\n",
        "model_params = {\n",
        "  \"layers\" : [4],\n",
        "  \"backbone\" : \"google/vit-base-patch16-224-in21k\",\n",
        "  \"f_coreset\" : 0.1\n",
        "}\n",
        "\n",
        "results = get_results(PatchCoreViT, model_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kPlbXGf4MLM1",
        "outputId": "a0476033-1158-460f-9c84-d2468cd58824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 4\n",
            "\n",
            "Class: bottle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 209/209 [00:11<00:00, 17.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4095/4095 [00:05<00:00, 772.13it/s]\n",
            "100%|██████████| 83/83 [00:04<00:00, 17.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 1.000\n",
            "Val: PIXEL Level ROCAUC: 0.988\n",
            "[INFO][evaluate] Image Level ROCAUC: 1.000\n",
            "[INFO][evaluate] Initial Score Threshold: 4.801 F1Score: 0.992\n",
            "[INFO][evaluate] Optimal Score Threshold: 4.711 F1Score: 1.000\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.017s\n",
            "\n",
            "Class: cable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO][__init__] Model PatchCore loaded on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [00:17<00:00, 12.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Coreset Subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4389/4389 [00:06<00:00, 731.39it/s]\n",
            "100%|██████████| 150/150 [00:13<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: IMAGE Level ROCAUC: 0.987\n",
            "Val: PIXEL Level ROCAUC: 0.981\n",
            "[INFO][evaluate] Image Level ROCAUC: 0.987\n",
            "[INFO][evaluate] Initial Score Threshold: 6.128 F1Score: 0.944\n",
            "[INFO][evaluate] Optimal Score Threshold: 6.038 F1Score: 0.952\n",
            "[INFO][evaluate] Average Inference time with batch_size=1: 0.025s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Layer 4\")\n",
        "print_results(results)\n",
        "result_json = save_json(results, \"development.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTxdgD5xNQ9m",
        "outputId": "614dc4cf-a392-401b-ce46-a1c6b42ceed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 4\n",
            "\n",
            "\n",
            "CLASS BREAKDOWN\n",
            "ROCAUC img: 1.000\tROCAUC pxl: 0.988\tf1_score: 1.000 \tbottle\n",
            "ROCAUC img: 0.987\tROCAUC pxl: 0.981\tf1_score: 0.952 \tcable\n",
            "\n",
            "SUMMARY\n",
            "Avg ROCAUC img: 0.993\n",
            "Avg ROCAUC pxl: 0.985\n",
            "Total Misclassified: 9\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q-AEp6kAcVNz"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23838b2c237a460da56d2e38751e804c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3e94b46157e4aea8c02c408f44bd4b0",
              "IPY_MODEL_d11e4451de6a44b485ae14448b26a31e",
              "IPY_MODEL_1dc17249a5a344bfb97cddc92b11c83a"
            ],
            "layout": "IPY_MODEL_3537e79038054ad8a14a269c1eca80c6"
          }
        },
        "c3e94b46157e4aea8c02c408f44bd4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34af98a144d42198c55edf54dd259ae",
            "placeholder": "​",
            "style": "IPY_MODEL_ffd7dadbe81a40108fa0a918006c9b4f",
            "value": "config.json: 100%"
          }
        },
        "d11e4451de6a44b485ae14448b26a31e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d10200e62fd24c8e8bdc8f14d2c4b433",
            "max": 502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_385b18f78a2743bcb6c9ec9f177ecd36",
            "value": 502
          }
        },
        "1dc17249a5a344bfb97cddc92b11c83a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18e7e7d959db4a94ac9d03465e1c801e",
            "placeholder": "​",
            "style": "IPY_MODEL_c960e9fe149c4377b665d33c9e4315bc",
            "value": " 502/502 [00:00&lt;00:00, 47.8kB/s]"
          }
        },
        "3537e79038054ad8a14a269c1eca80c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34af98a144d42198c55edf54dd259ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd7dadbe81a40108fa0a918006c9b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d10200e62fd24c8e8bdc8f14d2c4b433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385b18f78a2743bcb6c9ec9f177ecd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18e7e7d959db4a94ac9d03465e1c801e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c960e9fe149c4377b665d33c9e4315bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c19fd6c08e244afb9ead593af801e880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_778eaab52e2c4d9598596c6d67611bf1",
              "IPY_MODEL_8f8d649046f64c63a696ec85d29436aa",
              "IPY_MODEL_3cdf89789234494f9b3f209c7c2a4152"
            ],
            "layout": "IPY_MODEL_7779dce1d9b44857ba99874eecc6e224"
          }
        },
        "778eaab52e2c4d9598596c6d67611bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd763229c2ff4ab58eb6d9d4966c978a",
            "placeholder": "​",
            "style": "IPY_MODEL_a3d77da52d3745d78530ed02514c19db",
            "value": "model.safetensors: 100%"
          }
        },
        "8f8d649046f64c63a696ec85d29436aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ecc6c7e44bd48a19655f9175da1f4af",
            "max": 345579424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a47998d587fe49b48faa5ffb23d18fd4",
            "value": 345579424
          }
        },
        "3cdf89789234494f9b3f209c7c2a4152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ea5e774f0914b75bb12622ae017ef6d",
            "placeholder": "​",
            "style": "IPY_MODEL_18e36b1b2b5b48f0b8f86f47f6fc93fc",
            "value": " 346M/346M [00:06&lt;00:00, 60.1MB/s]"
          }
        },
        "7779dce1d9b44857ba99874eecc6e224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd763229c2ff4ab58eb6d9d4966c978a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d77da52d3745d78530ed02514c19db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ecc6c7e44bd48a19655f9175da1f4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47998d587fe49b48faa5ffb23d18fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ea5e774f0914b75bb12622ae017ef6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18e36b1b2b5b48f0b8f86f47f6fc93fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "406f126b1bff446d86b5a4f7dec8ea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f49e05852556416489674a6351a109ca",
              "IPY_MODEL_17b43a5e0e4e4af7827b649256ea4842",
              "IPY_MODEL_4d797d6b237b43cab8e287e0b5877609"
            ],
            "layout": "IPY_MODEL_5fd90c7d73414f1989293f0ae3f00290"
          }
        },
        "f49e05852556416489674a6351a109ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e173d007180c4f55a7b9f1cd70bd64e4",
            "placeholder": "​",
            "style": "IPY_MODEL_8f378a0ece9b4317b5717ef7665cd2e6",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "17b43a5e0e4e4af7827b649256ea4842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_775dcc565bd3459e8214d63d1d40512e",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6865f73dc0a948d3b2bfdb73cd1ecd20",
            "value": 160
          }
        },
        "4d797d6b237b43cab8e287e0b5877609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ca736408c6418984e958bce389a4fe",
            "placeholder": "​",
            "style": "IPY_MODEL_1c49f3a308f84763adb8dad5e7df04a5",
            "value": " 160/160 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "5fd90c7d73414f1989293f0ae3f00290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e173d007180c4f55a7b9f1cd70bd64e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f378a0ece9b4317b5717ef7665cd2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "775dcc565bd3459e8214d63d1d40512e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6865f73dc0a948d3b2bfdb73cd1ecd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00ca736408c6418984e958bce389a4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c49f3a308f84763adb8dad5e7df04a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}